---
title: 'Assignment #2: R code'
author: "Tristan De Alwis"
date: "4/16/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T, eval=T, tidy=T)
tidy.opts=list(width.cutoff=40)
```

# Loading Neccessary Libraries
```{r}
suppressPackageStartupMessages(library(MASS))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library('Rlab'))
suppressPackageStartupMessages(library(class))
```

# Q2
### Let lmabda be the traffic flow rate in vehicles per hour and M be the number of available toll

## (c)
```{r}
prior_m <- function(m){
  if (m ==8){
    return(4/7)
  }
  else if(m == 9){
    return(2/7)
  }
  else{
    return(1/7)
  }
}

q_m <- function(m, mNew){
  if (m == 9){
    return(1)
  }
  else if (mNew == 9){
    return(0.5)
  }
}

prior_lam <- function(lambda){
  return(1/((lambda+10) - max(c(0, lambda -10))))
}

get_lam <- function(lambdaOld){
  return(max(c(0, lambdaOld + runif(1, -10, +10), 1)))
}

q_lam <- function(lambdaOld, lambdaNew){
  if (lambdaOld== 0){
    return(abs(lambdaNew - 10)/20)
  }
  else{
    return(1/20)
  }
}

get_m <- function(mOld){
  if(mOld == 9){
    return(sample(c(8, 10), 1))
  }
  else {
    return(9)
  }
}

xDist <- function(x, lambda, m) {
  poisson <- 1 - exp(-1 * lambda * 15 * (1/3600)/m)
  return(max(dbinom(x, m, poisson, 1e-06)))
}

acceptanceProb <- function(lambdaNew, mNew, lambdaOld, mOld, X){
  pPrime <- xDist(X,lambdaNew, mNew)*prior_m(mNew)*prior_lam(lambdaNew)
  p <- xDist(X, lambdaOld, mOld) * prior_m(mOld)
  rNum <- pPrime * q_lam(lambdaOld, lambdaNew) * q_m(mOld, mNew)
  rDenom <- p * q_lam(lambdaNew, lambdaOld) * q_m(mNew, mOld)
  r <- rNum/rDenom
  
  return(min(1,r))
}
```

```{r}
MCMC <- function(M, x, ntrace, interval) {
  mOld <- M
  lambdaOld <- x*240
  
  Ms <- c(mOld)
  Lambdas <- c(lambdaOld)
  
  for (i in 1:ntrace){
    lambdaNew <- get_lam(lambdaOld)
    mNew <- get_m(mOld)
    
    if (acceptanceProb(lambdaNew, mNew, lambdaOld, mOld, x) >= runif(1, min=0, max=1)){
      mOld <- mNew
      lambdaOld <- lambdaNew
    }
    if (i%%interval ==0){
      Ms <- c(Ms, mOld)
      Lambdas <- c(Lambdas, lambdaOld)
    }
  }
  return(list('Ms'=Ms, 'Lambdas'=Lambdas))
}
```

```{r}
# (iv)
ntrace <- 5e6
interval = 1000
x2 <- MCMC(8,2,ntrace,interval)
```

```{r}
x4 <- MCMC(8,4,ntrace,interval)
```

```{r}
x6 <- MCMC(8,6,ntrace,interval)
```

```{r}
x8 <- MCMC(8,8,ntrace,interval)
```

```{r}
dF = c(x2[2],x4[2],x6[2],x8[2])
boxplot(dF, names = c('x2', 'x4', 'x6', 'x8'))
```

# Q3
```{r}
carsb = Cars93[,c(4,5,6,7,8,12,13,14,15,17,19:22,25,26)]
names(carsb)
carsb[,-16] = log(carsb[,-16])
```

## (a)
```{r}
lrfp = function(m) { (m[1,1]/(m[1,1]+m[2,1]))/(m[1,2]/(m[2,2]+m[1,2])) }
lrfn = function(m) { (m[2,1]/(m[1,1]+m[2,1]))/(m[2,2]/(m[2,2]+m[1,2])) }
f0 = function(m) {c(1-sum(diag(m))/sum(m),lrfp(m),lrfn(m))}

```


## (b)
```{r}
fit.lda= lda(Origin ~ .,
             data=carsb)

confusion.matrix.lda = table(predict(fit.lda)$class,carsb$Origin)

m = confusion.matrix.lda

vector = f0(m)
cat("CE:  ", vector[1], '\n')
cat("LR+: ", vector[2], '\n')
cat("LR-: ", vector[3], '\n')
```


## (c)
```{r}
fit.qda = qda(Origin ~ .,
             data=carsb)

confusion.matrix.qda = table(predict(fit.qda)$class,carsb$Origin)

m = confusion.matrix.qda

vector = f0(m)
cat("CE:  ", vector[1], '\n')
cat("LR+: ", vector[2], '\n')
cat("LR-: ", vector[3], '\n')
```
### Yes, the QDA appears to produce less error


## (d)
```{r}
fit.lda= lda(Origin ~ .,
             data=carsb,
             CV = TRUE)

confusion.matrix.lda = table(fit.lda$class,carsb$Origin)

m = confusion.matrix.lda

vector = f0(m)
cat("CE:  ", vector[1], '\n')
cat("LR+: ", vector[2], '\n')
cat("LR-: ", vector[3], '\n\n')

fit.qda = qda(Origin ~ .,
             data=carsb,
             CV = TRUE)

confusion.matrix.qda = table(fit.qda$class,carsb$Origin)

m = confusion.matrix.qda

vector = f0(m)
cat("CE:  ", vector[1], '\n')
cat("LR+: ", vector[2], '\n')
cat("LR-: ", vector[3], '\n')
```
#### lda() does better than qda() with CV because it is better at generalizing

# Q4
```{r}
pima1 = rbind(Pima.tr)[,c(2,3,4,5,6,8)]
names(pima1)
```


## (a)
```{r}
yes = length(which(pima1$type == 'Yes'))
yes <- yes/200
cat('yes: ', yes, '\n')

no = length(which(pima1$type == 'No'))
no <- no/200
cat('no:  ', no, '\n')
```
### The classifier would be incorrect 34% of the time.

## (b)
```{r}
knn.function = function(k.list, xtrain, gr) {

  pr.tab = matrix(NA,length(k.list),3)
  for (i in 1:length(k.list)) {

    knn.fit = knn.cv(xtrain, gr, k=k.list[i], use.all=TRUE)
    cm = table(knn.fit,gr)
    # print(cm)
    pr.tab[i,] = f0(cm)
    }
  cm
  return(pr.tab)
}
```


## (c)
```{r}
k.list = seq(1, 125, 2)
cv.tab = knn.function(k.list, pima1[,1:5], pima1[,6])
cv.tab
cat("Min C.E.: ", min(cv.tab[,1]), '\n')
cat("Min LR+: ", min(cv.tab[,2]), '\n')
cat("Max LR+: ", max(cv.tab[,2]), '\n')
cat("Min LR-: ", min(cv.tab[,3], na.rm=TRUE), '\n')
cat("Max LR-: ", max(cv.tab[,3], na.rm=TRUE), '\n')

# plot(seq(1, 125, 2), cv.tab[,1], pch=16, ylab="Classification Error", xlab="K", main="Normalized Features")
# lines(seq(1, 125, 2), cv.tab[,1], lty=1)
# abline(h = 0.34, lty = 2, lwd = 3, col = 'dark blue')
```
### Using only odd numbers for K avoids the need to break ties within the classification algorithm. The min value of CE is .240 for K = 32. The minumum LR+

## (d)
```{r}
norm_feat <- apply(pima1[-6], 2, function(x) { return((x - mean(x))/sd(x)) } )

k.list = seq(1, 125, 2)
cv.tab = knn.function(k.list, norm_feat, pima1[,6])
cat("Min C.E.: ", min(cv.tab[,1]), '\n')
cat("Min LR+: ", min(cv.tab[,2]), '\n')
cat("Max LR+: ", max(cv.tab[,2]), '\n')
cat("Min LR-: ", min(cv.tab[,3], na.rm=TRUE), '\n')
cat("Max LR-: ", max(cv.tab[,3], na.rm=TRUE), '\n')

knn_mat <- knn.function(k.list, norm_feat, pima1[,6])
plot(seq(1, 125, 2), knn_mat[,1], pch=16, ylab='Classification Error', xlab = 'K', main = 'Normalized Features')
lines(seq(1, 125, 2), knn_mat[,1], lty=1)
abline(h = 0.34, lty = 2, lwd = 3, col = 'dark blue')
```
### Normalize reduces the scale of the variables resulting in reduced error.

# Q5
## (a)
```{r}
lambda_bim <- function(lam, X, m = 10, t = 15/3600) {
  return(log(choose(m, X) * (1 - exp((-lam*t)/m))^(X) * (exp((-lam*t)/m)^(m-X))))
}

probs <- c()
ind <- seq(500, 700, 1)
for (i in ind){
  probs <- c(probs, lambda_bim(i, X=6))
}

plot(ind, probs)
```

## (b)
```{r}
lam_mle <- function(x, M=10, t=(15/3600)){
  (M/t)*log(M/(M-x))
}

for (i in 0:9){
  sprintf('MLE for x=%s: %s', i, round(lam_mle(i), 3)) %>% print(.)
}
```


## (c)
```{r}
for (i in 0:10){
  sprintf('MLE for x=%s: %s', i, round(lam_mle(i), 3)) %>% print(.)
}
```
### Can't divide by 0 so can't give all estimates